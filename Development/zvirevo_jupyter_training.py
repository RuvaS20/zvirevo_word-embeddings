# -*- coding: utf-8 -*-
"""zvirevo_jupyter_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ybZyCNqLTTiNEe-nrW4QHBCDHuSsA4i
"""

from google.colab import drive
drive.mount('/content/drive')

path_to_og_data = '/content/drive/MyDrive/data2.csv'

"""## **Data Preprocessing**

### **Loading the data**
"""

# Loading data
import pandas as pd
df = pd.read_csv(path_to_og_data)

df.head(20)

df.info()

"""### **Dropping duplicates and null's**"""

df.drop_duplicates(inplace=True)

df.info()

df['Text'].isnull().sum()

df = df.dropna(subset=['Text'])

df['Text'].isnull().sum()

"""### **Cleaning and normalising**"""

#cleaning text
import unicodedata
import re

def remove_special_characters(text):
    if isinstance(text, str):
        # Remove all special characters except whitespace
        return re.sub(r'[^\w\s]', '', text)
    return text

def remove_numbers(text):
    if isinstance(text, str):
        # Remove all numbers
        return re.sub(r'\d+', '', text)
    return text

def remove_single_letter_words(text):
    if isinstance(text, str):
        # Remove single-letter words from the text
        return ' '.join(word for word in text.split() if len(word) > 1)
    return text

def trim_whitespace(text):
    if isinstance(text, str):
        # Strip leading and trailing whitespace
        return text.strip()
    return text

def lower_text(text):
    text = text.lower()  # Convert to lowercase
    return text

def remove_superscripts(text):
    if isinstance(text, str):
        # Regular expression to match superscript characters including numbers and letters
        superscript_pattern = r'[¹²³⁴⁵⁶⁷⁸⁹⁰ʰʲʳˢˣ]'
        # Remove superscript characters
        return re.sub(superscript_pattern, '', text)
    return text

def clean_text(text):
    # remove_words_with_diacritics(text)
    text = remove_numbers(text)
    text = remove_special_characters(text)
    text = remove_single_letter_words(text)
    text = trim_whitespace(text)
    text = lower_text(text)
    text = remove_superscripts(text)
    return text

df['Text'] = df['Text'].apply(clean_text)

df.head(20)

df.iloc[57]

df.to_csv("final_data.csv")

df = pd.read_csv("final_data.csv")

"""### **Tokenization**"""

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

def tokenize_text(text):
    return word_tokenize(text)

df['Tokens'] = df['Text'].apply(tokenize_text)

df.head()

df.info()

"""## **Training**

### **Word2Vec Training**
"""

from gensim.models import Word2Vec

tokenized_text = df["Tokens"].tolist()  # Prepare data for Word2Vec

# Initialize the Word2Vec model
w2v_model1 = gensim.models.Word2Vec(
    vector_size=300,  # Set the vector size
    window=5,       # Set the context window size
    min_count=2,     # Set the minimum count
    sg=0,            # Use skip-gram; set to 0 for CBOW
    workers=2,       # Number of worker threads to use
    epochs=7        # Number of epochs to train
)

# w2v_model1.save("w2v_model1.model")

# Initialize and train Word2Vec model 2
w2v_model2 = Word2Vec(
    sentences=tokenized_text,
    vector_size=300,
    window=3,
    min_count=2,
    workers=4,
    sg=0,  # CBOW
    hs=0,  # Use negative sampling
    negative=5,
    ns_exponent=0.75,
    alpha=0.075,
    min_alpha=0.0075,
    epochs=10
)
# w2v_model2.save("w2v_model2.model")

# Initialize and train Word2Vec model 3
w2v_model3 = Word2Vec(
    sentences=tokenized_text,
    vector_size=200,
    window=5,
    min_count=2,
    workers=4,
    sg=0,  # CBOW
    hs=0,  # Use negative sampling
    negative=5,
    ns_exponent=0.75,
    alpha=0.1,
    min_alpha=0.01,
    epochs=10
)
# w2v_model3.save("w2v_model3.model")

# Initialize and train Word2Vec model 4
w2v_model4 = Word2Vec(
    sentences=tokenized_text,
    vector_size=200,
    window=3,
    min_count=2,
    workers=4,
    sg=0,  # CBOW
    hs=0,  # Use negative sampling
    negative=5,
    ns_exponent=0.75,
    alpha=0.1,
    min_alpha=0.01,
    epochs=5
)
# w2v_model4.save("w2v_model4.model")

"""### **FastText Training**"""

tokenized_text = df["Tokens"].tolist()  # Prepare data for fastText

# Initialize and train FastText model 1
ft_model1 = FastText(
  vector_size=300, window=5, min_count=2, workers=4, sg=0,
  hs=0, negative=5, ns_exponent=0.75, alpha=0.1, min_alpha=0.01, epochs=10
)
ft_model1.build_vocab(tokenized_text)
ft_model1.train(tokenized_text, total_examples=ft_model1.corpus_count, epochs=ft_model1.epochs)
# ft_model1.save("ft_model1.model")



# Initialize and train FastText model 2
ft_model2 = FastText(
  vector_size=300, window=3, min_count=2, workers=4, sg=0,
  hs=0, negative=5, ns_exponent=0.75, alpha=0.075, min_alpha=0.0075, epochs=10
)
ft_model2.build_vocab(tokenized_text)
ft_model2.train(tokenized_text, total_examples=ft_model2.corpus_count, epochs=ft_model2.epochs)
# ft_model2.save("ft_model2.model")



# Initialize and train FastText model 3
ft_model3 = FastText(
  vector_size=200, window=5, min_count=2, workers=4, sg=0,
  hs=0, negative=5, ns_exponent=0.75, alpha=0.1, min_alpha=0.01, epochs=10
)
ft_model3.build_vocab(tokenized_text)
ft_model3.train(tokenized_text, total_examples=ft_model3.corpus_count, epochs=ft_model3.epochs)
# ft_model3.save("ft_model3.model")


  # Initialize and train FastText model 4
ft_model4 = FastText(
  vector_size=200, window=3, min_count=2, workers=4, sg=0,
  hs=0, negative=5, ns_exponent=0.75, alpha=0.1, min_alpha=0.01, epochs=5
)
ft_model4.build_vocab(tokenized_text)
ft_model4.train(tokenized_text, total_examples=ft_model4.corpus_count, epochs=ft_model4.epochs)
# ft_model4.save("ft_model3.model")

"""## **Testing**

### **Word2Vec Testing**
"""

# Words to test similarity
words = ['mukadzi', 'murume', 'mwana', 'mutongi', 'amai', 'munhu', 'zvakanaka', 'imba', 'ruvara', 'dhokotera', 'mambo', 'harare', 'nyika']

# Assuming you have loaded your Word2Vec models as w2v_model1, w2v_model2, etc.

for word in words:
    print('--------------------------------------------------------')

    try:
        w2v_words1 = w2v_model1.most_similar(word, topn=10)
    except KeyError:
        w2v_words1 = []
        print(f"Word '{word}' not found in Word2Vec Model 1 vocabulary.")

    try:
        w2v_words2 = w2v_model2.most_similar(word, topn=10)
    except KeyError:
        w2v_words2 = []
        print(f"Word '{word}' not found in Word2Vec Model 2 vocabulary.")

    try:
        w2v_words3 = w2v_model3.most_similar(word, topn=10)
    except KeyError:
        w2v_words3 = []
        print(f"Word '{word}' not found in Word2Vec Model 3 vocabulary.")

    try:
        w2v_words4 = w2v_model4.most_similar(word, topn=10)
    except KeyError:
        w2v_words4 = []
        print(f"Word '{word}' not found in Word2Vec Model 4 vocabulary.")

    print(f'\nWORD2VEC MODEL 1: Words similar to {word.upper()}\n')
    for sim in w2v_words1:
        print(sim)

    print(f'\nWORD2VEC MODEL 2: Words similar to {word.upper()}\n')
    for sim in w2v_words2:
        print(sim)

    print(f'\nWORD2VEC MODEL 3: Words similar to {word.upper()}\n')
    for sim in w2v_words3:
        print(sim)

    print(f'\nWORD2VEC MODEL 4: Words similar to {word.upper()}\n')
    for sim in w2v_words4:
        print(sim)

# Save the trained Word2Vec model
# w2v_model2.save("w2v_model2.model")

"""### **Fasttext Testing**"""

# Words to test similarity
words = ['mukadzi', 'murume', 'mwana', 'mutongi', 'amai', 'munhu', 'zvakanaka', 'imba', 'ruvara', 'dhokotera', 'mambo', 'harare', 'nyika']

for word in words:
    print('--------------------------------------------------------')

    f_words1 = ft_model1.get_nearest_neighbors(word)
    f_words2 = ft_model2.get_nearest_neighbors(word)
    f_words3 = ft_model3.get_nearest_neighbors(word)
    f_words4 = ft_model4.get_nearest_neighbors(word)

    print(f'\nFASTTEXT MODEL 1: Words similar to {word.upper()}\n')
    for sim2 in f_words1:
        print(sim2)

    print(f'\nFASTTEXT MODEL 2: Words similar to {word.upper()}\n')
    for sim2 in f_words2:
        print(sim2)

    print(f'\nFASTTEXT MODEL 3: Words similar to {word.upper()}\n')
    for sim2 in f_words3:
        print(sim2)

    print(f'\nFASTTEXT MODEL 4: Words similar to {word.upper()}\n')
    for sim2 in f_words4:
        print(sim2)

#saving the fasttext model
# ft_model1.save("ft_model1.model")